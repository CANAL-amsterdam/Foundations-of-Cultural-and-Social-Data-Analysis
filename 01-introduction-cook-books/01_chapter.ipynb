{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CANAL-amsterdam/Foundations-of-Cultural-and-Social-Data-Analysis/blob/main/01-introduction-cook-books/01_chapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3cbf3f",
      "metadata": {
        "id": "db3cbf3f"
      },
      "outputs": [],
      "source": [
        "!pip install \"numpy<2,>=1.13\" \"pandas~=1.1\" \"matplotlib<4,>=2.1\" \"scipy<2,>=0.18\" \"scikit-learn>=0.19\" \"mpl-axes-aligner<2,>=1.1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CANAL-amsterdam/Foundations-of-Cultural-and-Social-Data-Analysis"
      ],
      "metadata": {
        "id": "Th_uRHzu5WbS"
      },
      "id": "Th_uRHzu5WbS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Foundations-of-Cultural-and-Social-Data-Analysis/01-introduction-cook-books"
      ],
      "metadata": {
        "id": "Xhh2pp_i55-H"
      },
      "id": "Xhh2pp_i55-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "mEo3sJKP5_JH"
      },
      "id": "mEo3sJKP5_JH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ed68a505",
      "metadata": {
        "id": "ed68a505"
      },
      "source": [
        "(chp-introduction-cook-books)=\n",
        "# Introduction\n",
        "\n",
        "## Quantitative Data Analysis and the Humanities\n",
        "\n",
        "The use of quantitative methods in humanities disciplines such as history, literary\n",
        "studies, and musicology has increased considerably in recent years. Now it is not uncommon\n",
        "to learn of a historian using geospatial data, a literary scholar applying techniques from\n",
        "computational linguistics, or a musicologist employing pattern matching methods. Similar\n",
        "developments occur in humanities-adjacent disciplines, such as archeology, anthropology,\n",
        "and journalism. An important driver of this development, we suspect, is the advent of\n",
        "cheap computational resources as well as the mass digitization of libraries and archives\n",
        "{cite:p}`Imai:2018,abello2012computational,borgman2010scholarship,vanKranenburg:2017` It\n",
        "has become much more common in humanities research to analyze thousands, if not millions,\n",
        "of documents, objects, or images; an important part of the reason why quantitative methods\n",
        "are attractive now is that they promise the means to detect and analyze patterns in these\n",
        "large collections.\n",
        "\n",
        "A recent example illustrating the promise of data-rich book history and cultural analysis\n",
        "is {cite:t}`bode2012reading`. Bode's analysis of the massive online bibliography of Australian\n",
        "literature *AusLit* demonstrates how quantitative methods can be used to enhance our\n",
        "understanding of literary history in ways that would not be possible absent data-rich and\n",
        "computer-enabled approaches. Anchored in the cultural materialist focus of Australian\n",
        "literary studies, Bode uses data analysis to reveal unacknowledged\n",
        "shifts in the demographics of Australian novelists, track the entry of British publishers\n",
        "into the Australian book market in the 1890s, and identify ways Australian\n",
        "literary culture departed systematically from British practices. A second enticing example\n",
        "showing the potential of data-intensive research is found in {cite:t}`dasilva:2016`. Using\n",
        "data from large-scale folklore databases, {cite:t}`dasilva:2016` investigate the\n",
        "international spread of folktales. Based on quantitative analyses, they show how the\n",
        "diffusion of folktales is shaped by language, population histories, and migration. A third and final example---one that can\n",
        "be considered a landmark in the computational analysis of literary texts---is the\n",
        "influential monograph by {cite:t}`burrows:1987` on Jane Austen's oeuvre. Burrows uses relatively\n",
        "simple statistics to analyze the frequencies of\n",
        "inconspicuous, common words that typically escape the eye of the human reader. In doing so he\n",
        "documents hallmarks of Austen's sentence style and carefully documents differences in characters' speaking styles. The\n",
        "book illustrates how quantitative analyses can yield valuable and lasting insights into\n",
        "literary texts, even if they are not applied to datasets that contain millions of texts.\n",
        "\n",
        "Although recent interest in quantitative analysis may give the impression that humanities\n",
        "scholarship has entered a new era, we should not forget that it is part of a development\n",
        "that began much earlier. In fact, for some, the ever so prominent \"quantitative turn\" we\n",
        "observe in humanities research nowadays is not a new feature of humanities scholarship; it\n",
        "marks a return to established practice. The use of quantitative methods such as linear\n",
        "regression, for example, was a hallmark of social history in the 1960s and 1970s\n",
        "{cite}`sewelljr2005political`. In literary studies, there are numerous examples of\n",
        "quantitative methods being used to explore the social history of literature\n",
        "{cite}`williams1961long,escarpit:1958` and to study the literary style of individual\n",
        "authors {cite}`yule:1944,muller1967etude`. Indeed, the founder of \"close reading,\" I. A.\n",
        "Richards, was himself concerned with the analysis and use of word frequency lists\n",
        "{cite}`igarashi2015statistical`.\n",
        "\n",
        "Quantitative methods fell out of favor in the 1980s as interest in cultural history\n",
        "displaced interest in social history (where quantitative methods had been indispensable).\n",
        "This realignment of research priorities in history is known as \"the <span\n",
        "class=\"index\">cultural turn</span>.\" In his widely circulated account, William Sewell\n",
        "offers two reasons for his and his peers' turn away from social history and quantitative\n",
        "methods in the 1970s. First, \"latent ambivalence\" about the use of quantitative methods\n",
        "grew in the 1960s because of their association with features of society that were regarded\n",
        "as defective by students in the 1960s. Quantitative methods were associated with\n",
        "undesirable aspects of what Sewell labels \"the Fordist mode of socioeconomic regulation,\"\n",
        "including repressive standardization, big science, corporate conformity, and state\n",
        "bureaucracy. Erstwhile social historians like Sewell felt that \"in adopting quantitative\n",
        "methodology we were participating in the bureaucratic and reductive logic of big science,\n",
        "which was part and parcel of the system we wished to criticize\"\n",
        "[{cite:author}`sewelljr2005political` {cite:year}`sewelljr2005political`, 180-81]. Second, the \"abstracted empiricism\" of\n",
        "quantitative methods was seen as failing to give adequate attention to questions of human\n",
        "agency and the texture of experience, questions which cultural history focused on\n",
        "[{cite:author}`sewelljr2005political` {cite:year}`sewelljr2005political`, 182].\n",
        "\n",
        "We make no claims about the causes of the present revival of interest in\n",
        "quantitative methods. Perhaps it has something to do with previously dominant\n",
        "methods in the humanities, such as critique and close reading, \"running out of\n",
        "steam\" in some sense, as {cite:t}`latour2004why` has suggested.  This would go some way\n",
        "towards explaining why researchers are now (re)exploring quantitative\n",
        "approaches. Or perhaps the real or perceived costs associated with the use of\n",
        "quantitative methods have declined to a point that the potential benefits\n",
        "associated with their use---for many, broadly the same as they were in the\n",
        "1960s---now attract researchers.\n",
        "\n",
        "What is clear, however, is that university curricula in the humanities do not\n",
        "at present devote sufficient time to thoroughly acquaint and involve students\n",
        "with data-intensive and quantitative research, making it challenging for\n",
        "humanities students and scholars to move from spectatorship to active\n",
        "participation in (discussions surrounding) quantitative research. The aim of\n",
        "this book, then, is precisely to accommodate humanities students and scholars\n",
        "in their growing desire to understand how to tackle theoretical and descriptive\n",
        "questions using data-rich, computer-assisted approaches.\n",
        "\n",
        "Through several case studies, this book offers a guide to quantitative data analysis using\n",
        "the Python programming language. The Python language is widely used in academia, industry,\n",
        "and the public sector. It is the official programming language in secondary education\n",
        "in France and the most widely taught programming language in\n",
        "US universities {cite}`ministere2018projets,guo2014python`. If learning\n",
        "data carpentry in Python chafes, you may rest assured that improving your fluency in\n",
        "Python is likely to be worthwhile. In this book, we do not focus on learning how\n",
        "to code per se; rather, we wish to highlight how quantitative methods can be\n",
        "meaningfully applied in the particular context of humanities scholarship. The book\n",
        "concentrates on textual data analysis, because decades of research have been devoted to\n",
        "this domain and because current research remains vibrant. Although many research\n",
        "opportunities are emerging in music, audio, and image analysis, they fall outside the\n",
        "scope of the present undertaking\n",
        "{cite:p}`clarke:2004,tzanetakis:2007,cook:2013,clement2016measured`. All chapters focus\n",
        "on real-world data sets throughout and aim to\n",
        "illustrate how quantitative data analysis can play more than an auxiliary role in tackling\n",
        "relevant research questions in the humanities.\n",
        "\n",
        "## Overview of the Book\n",
        "\n",
        "This book is organized into two parts. Part 1 covers essential techniques for gathering,\n",
        "cleaning, representing, and transforming textual and tabular data. \"Data carpentry\"---as\n",
        "the collection of these techniques is sometimes referred to---precedes any effort to\n",
        "derive meaningful insights from data using quantitative methods. The four chapters of Part\n",
        "1 prepare the reader for the data analyses presented in the second part of this book.\n",
        "\n",
        "To give an idea of what a complete data analysis entails, the current chapter presents an\n",
        "exploratory data analysis of historical cookbooks. In a nutshell, we demonstrate which\n",
        "steps are required for a complete data analysis, and how Python facilitates the\n",
        "application of these steps. After sketching the main ingredients of quantitative data\n",
        "analysis, we take a step back in chapter {ref}`chp-getting-data` to describe essential\n",
        "techniques for data gathering and exchange. Built around a case study of extracting and\n",
        "visualizing the social network of the characters in Shakespeare's *Hamlet*, the chapter\n",
        "provides a detailed introduction into different models of data exchange, and how Python\n",
        "can be employed to effectively gather, read, and store different data formats, such as CSV,\n",
        "JSON, PDF, and XML. Chapter {ref}`chp-vector-space-model` builds on chapter\n",
        "{ref}`chp-getting-data`, and focuses on the question of how texts can be represented for\n",
        "further analysis, for instance for document comparison. One powerful form of\n",
        "representation that allows such comparisons is the so-called \"Vector Space Model\". The\n",
        "chapter provides a detailed manual for how to construct document-term matrices from word\n",
        "frequencies derived from text documents. To illustrate the potential and benefits of the\n",
        "Vector Space Model, the chapter analyzes a large corpus of classical French drama, and\n",
        "shows how this representation can be used to quantitatively assess similarities and\n",
        "distances between texts and subgenres. While data analysis in, for example, literary\n",
        "studies, history and folklore is often focused on text documents, subsequent analyses\n",
        "often require processing and analyzing tabular data. The final chapter of part 1\n",
        "(chapter {ref}`chp-working-with-data`) provides a detailed introduction into how such\n",
        "tabular data can be processed using the popular data analysis library \"Pandas\". The\n",
        "chapter centers around diachronic developments in child naming practices, and demonstrates\n",
        "how Pandas can be efficiently employed to quantitatively describe and visualize long-term\n",
        "shifts in naming. All topics covered in Part 1 should be accessible to everyone who has\n",
        "had some prior exposure to programming.\n",
        "\n",
        "Part 2 features more detailed and elaborate examples of data analysis using\n",
        "Python. Building on knowledge from chapter {ref}`chp-working-with-data`, the first chapter\n",
        "of part 2 (chapter {ref}`chp-statistics-essentials`) uses the Pandas library to\n",
        "statistically describe responses to a questionnaire about the reading of literature and\n",
        "appreciation of classical music. The chapter provides detailed descriptions of important\n",
        "summary statistics, allowing to analyze whether, for example, differences between\n",
        "responses can be attributed to differences between certain demographics. Chapter\n",
        "{ref}`chp-statistics-essentials` paves the way for the introduction to probability in\n",
        "chapter {ref}`chp-intro-probability`. This chapter revolves around the classic case of\n",
        "disputed authorship of several essays in *The Federalist Papers*, and demonstrates how probability theory\n",
        "and Bayesian inference in particular can be applied to shed light on this still intriguing\n",
        "case. Chapter {ref}`chp-map-making` discusses a series of fundamental techniques to create\n",
        "geographic maps with Python. The chapter analyzes a dataset describing important battles\n",
        "fought during the American Civil War. Using narrative mapping techniques, the chapter\n",
        "provides insight into the trajectory of the war. After this brief intermezzo, chapter\n",
        "{ref}`chp-stylometry` returns to the topic of disputed authorship, providing a more\n",
        "detailed and thorough overview of common and essential techniques used to model the\n",
        "writing style of authors. The chapter aims to reproduce a stylometric analysis revolving\n",
        "around a challenging authorship controversy from the twelfth century. On the basis of a\n",
        "series of different stylometric techniques (including Burrows's Delta, Agglomerative\n",
        "Hierarchical Clustering, and Principal Component Analysis), the chapter illustrates how\n",
        "quantitative approaches aid to objectify intuitions about document authenticity. The\n",
        "closing chapter of part 2 (chapter {ref}`chp-topic-models`) connects the preceding\n",
        "chapters, and challenges the reader to integrate the learned data analysis techniques as\n",
        "well as to apply them to a case about trends in decisions issued by the United States\n",
        "Supreme Court. The chapter provides a detailed account of mixed-membership models or\n",
        "\"topic models\", and employs these to make visible topical shifts in the Supreme Court's\n",
        "decision-making. Note that the different chapters in part 2 make different assumptions\n",
        "about readers' background preparation. Chapter {ref}`chp-intro-probability` on disputed\n",
        "authorship, for example, will likely be easier for readers who have some familiarity with\n",
        "probability and statistics. Each chapter begins with a discussion of the background\n",
        "assumed.\n",
        "\n",
        "## Related Books\n",
        "\n",
        "Our monograph aims to fill a specific lacuna in the field, as a coherent, book-length discussion of Python programming for data analysis in the humanities. To manage the expectations of our readership, we believe it is useful to state how this book wants to position itself against some of the existing literature in the field, with which our book inevitably intersects and overlaps. For the sake of brevity, we limit ourselves to more recent work. At the start, it should be emphasized that other resources than the traditional monograph also play a vital role in the community surrounding quantitative work in the humanities. The (multilingual) website [The Programming Historian](https://programminghistorian.org/), for instance, is a tutorial platform that hosts a rich variety of introductory lessons that target specific data-analytic skills {cite:p}`ph2019`.\n",
        "\n",
        "The focus on Python distinguishes our work from a number of recent textbooks that use the programming language R {cite:p}`R13`, a robust and mature scripting platform for statisticians that is also used in the social sciences and humanities. A general introduction to data analysis using R can be found in {cite:t}`wickham:2017`.  One can also consult {cite:t}`jockers:2014` or {cite:t}`arnold2015`, which have humanities scholars as their intended audience. Somewhat related are two worthwhile textbooks on corpus and quantitative linguistics, {cite:t}`baayen:2008` and {cite:t}`gries2013`, but these are less accessible to an audience outside of linguistics. There also exist some excellent more general introductions to the use of Python for data analysis, such as {cite:t}`mckinney2017` and {cite:t}`vanderplas:2016`. These handbooks are valuable resources in their own respect but they have the drawback that they do not specifically cater to researchers in the humanities. The exclusive focus on Humanities data analysis clearly sets our book apart from these textbooks---which the reader might nevertheless find useful to consult at a later stage.\n",
        "\n",
        "## How to Use This Book\n",
        "\n",
        "This book has a practical approach, in which descriptions and explanations of quantitative\n",
        "methods and analyses are alternated with concrete implementations in programming code. We\n",
        "strongly believe that such a hands-on approach stimulates the learning process, enabling\n",
        "researchers to apply and adopt the newly acquired knowledge to their own research\n",
        "problems. While we generally assume a linear reading process, all chapters are constructed\n",
        "in such a way that they *can* be read independently, and code examples are not dependent on\n",
        "implementations in earlier chapters. As such, readers familiar with the principles and\n",
        "techniques of, for instance, data exchange or manipulating tabular data, may safely skip\n",
        "chapters {ref}`chp-getting-data` and {ref}`chp-working-with-data`.\n",
        "\n",
        "The remainder of this chapter, like all the chapters in this book, includes Python code\n",
        "which you should be able to execute in your computing environment. All code presented here\n",
        "assumes your computing environment satisfies the basic requirement of having an\n",
        "<span class=\"index\">installation</span> of Python (version 3.9 or higher) available on a Linux, macOS, or Microsoft\n",
        "Windows system. A distribution of Python may be obtained from the [Python Software\n",
        "Foundation](https://www.python.org/) or through the operating system's package manager\n",
        "(e.g., `apt` on Debian-based Linux, or `brew` on macOS). Readers new to Python may wish to install the\n",
        "[<span class=\"index\">Anaconda</span>](https://www.continuum.io/) Python distribution which bundles most of the Python\n",
        "packages used in this book. We recommend that macOS and Windows users, in particular, use\n",
        "this distribution.\n",
        "\n",
        "### What you should know\n",
        "As said, this is not a book teaching how to program from scratch, and we assume the reader\n",
        "already has some working knowledge about programming and Python. However, we do not expect\n",
        "the reader to have mastered the language. A relatively short introduction to programming\n",
        "and Python will be enough to follow along (see, for example, *Python Crash Course* by\n",
        "{cite:t}`matthes:2016`). The following code blocks serve as a refresher of some important\n",
        "programming principles and aspects of Python. At the same time, they allow you to test\n",
        "whether you know enough about Python to start this book. We advise you to execute these\n",
        "examples as well as all code blocks in the rest of the book in so-called \"<span\n",
        "class=\"index\">Jupyter</span> notebooks\" (see https://jupyter.org/). Jupyter notebooks\n",
        "offer a wonderful environment for executing code, writing notes, and creating\n",
        "visualizations. The code in this book is assigned the DOI ``10.5281/zenodo.3563075``, and\n",
        "can be downloaded from https://doi.org/10.5281/zenodo.3563075.\n",
        "\n",
        "#### Variables\n",
        "First of all, you should know that variables are defined using the assignment operator\n",
        "`=`. For example, to define the variable `x` and assign the value `100` to it, we write:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af9413d",
      "metadata": {
        "id": "1af9413d"
      },
      "outputs": [],
      "source": [
        "x = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e97258",
      "metadata": {
        "id": "15e97258"
      },
      "source": [
        "Numbers, such as `1`, `5`, and `100` are called integers and are of type `int` in\n",
        "Python. Numbers with a fractional part (e.g., `9.33`) are of the type `float`. The string\n",
        "data type (`str`) is commonly used to represent text. Strings can be expressed in multiple\n",
        "ways: they can be enclosed with single or double quotes. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9cbfde2",
      "metadata": {
        "id": "f9cbfde2"
      },
      "outputs": [],
      "source": [
        "saying = \"It's turtles all the way down\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5796c4",
      "metadata": {
        "id": "2c5796c4"
      },
      "source": [
        "#### Indexing sequences\n",
        "Essentially, Python strings are sequences of characters, where characters are strings of\n",
        "length one. Sequences such as strings can be indexed to retrieve any component character\n",
        "in the string. For example, to retrieve the first character of the string defined above,\n",
        "we write the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5cfe344",
      "metadata": {
        "id": "b5cfe344"
      },
      "outputs": [],
      "source": [
        "print(saying[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ddcad7e",
      "metadata": {
        "id": "6ddcad7e"
      },
      "source": [
        "Note that like many other programming languages, Python starts counting from zero, which\n",
        "explains why the first character of a string is indexed using the number 0. We use the\n",
        "function `print()` to print the retrieved value to our screen.\n",
        "\n",
        "#### Looping\n",
        "You should also know about the concept of \"looping\". Looping involves a sequence of Python\n",
        "instructions, which is repeated until a particular condition is met. For example, we might\n",
        "loop (or iterate as it's sometimes called) over the characters in a string and print each\n",
        "character to our screen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84434259",
      "metadata": {
        "id": "84434259"
      },
      "outputs": [],
      "source": [
        "string = \"Python\"\n",
        "for character in string:\n",
        "    print(character)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b69a9f",
      "metadata": {
        "id": "89b69a9f"
      },
      "source": [
        "#### Lists\n",
        "Strings are sequences of characters. Python provides a number of other sequence types,\n",
        "allowing us to store different data types. One of the most commonly used sequence types is\n",
        "the `list`. A list has similar properties as strings, but allows us to store any kind of\n",
        "data type inside:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e452c4e",
      "metadata": {
        "id": "8e452c4e"
      },
      "outputs": [],
      "source": [
        "numbers = [1, 1, 2, 3, 5, 8]\n",
        "words = [\"This\", \"is\", \"a\", \"list\", \"of\", \"strings\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e35c67",
      "metadata": {
        "id": "01e35c67"
      },
      "source": [
        "We can index and slice lists using the same syntax as with strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8abb2717",
      "metadata": {
        "id": "8abb2717"
      },
      "outputs": [],
      "source": [
        "print(numbers[0])\n",
        "print(numbers[-1])  # use -1 to retrieve the last item in a sequence\n",
        "print(words[3:])  # use slice syntax to retrieve a subsequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d57e3a",
      "metadata": {
        "id": "91d57e3a"
      },
      "source": [
        "#### Dictionaries and sets\n",
        "Dictionaries (`dict`) and sets (`set`) are unordered data types in Python. Dictionaries\n",
        "consist of entries, or \"keys\", that hold a value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3577106b",
      "metadata": {
        "id": "3577106b"
      },
      "outputs": [],
      "source": [
        "packages = {'matplotlib': 'Matplotlib is a Python 2D plotting library',\n",
        "            'pandas': 'Pandas is a Python library for data analysis',\n",
        "            'scikit-learn': 'Scikit-learn helps with Machine Learning in Python'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda2b806",
      "metadata": {
        "id": "fda2b806"
      },
      "source": [
        "The keys in a dictionary are unique and unmutable. To look up the value of a given key, we\n",
        "\"index\" the dictionary using that key, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d46b29b5",
      "metadata": {
        "id": "d46b29b5"
      },
      "outputs": [],
      "source": [
        "print(packages['pandas'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f561dee",
      "metadata": {
        "id": "6f561dee"
      },
      "source": [
        "Sets represent unordered collections of unique, unmutable objects. For example, the\n",
        "following code block defines a set of strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdc5ddc",
      "metadata": {
        "id": "3fdc5ddc"
      },
      "outputs": [],
      "source": [
        "packages = {\"matplotlib\", \"pandas\", \"scikit-learn\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13dbdc60",
      "metadata": {
        "id": "13dbdc60"
      },
      "source": [
        "#### Conditional expressions\n",
        "We expect you to be familiar with conditional expressions. Python provides the statements\n",
        "`if`, `elif`, and `else`, which are used for conditional execution of certain lines of\n",
        "code. For instance, say we want to print all strings in a list that contain the letter\n",
        "*i*. The `if` statement in the following code block executes the print function *on the\n",
        "condition* that the current string in the loop contains the string *i*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "796a702f",
      "metadata": {
        "id": "796a702f"
      },
      "outputs": [],
      "source": [
        "words = [\"move\", \"slowly\", \"and\", \"fix\", \"things\"]\n",
        "for word in words:\n",
        "    if \"i\" in word:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61afbde0",
      "metadata": {
        "id": "61afbde0"
      },
      "source": [
        "#### Importing modules\n",
        "\n",
        "```{margin}\n",
        "For an overview of all packages and modules in Python's standard library, see\n",
        "https://docs.python.org/3/library/. For an overview of the various built-in functions,\n",
        "see https://docs.python.org/3/library/functions.html.\n",
        "```\n",
        "\n",
        "Python provides a tremendous range of additional functionality through modules in its\n",
        "<span class=\"index\">standard library</span>. We assume you know about the concept of \"importing\" modules and\n",
        "packages, and how to use the newly imported functionality. For example, to import the\n",
        "model `math`, we write the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ceaefc",
      "metadata": {
        "id": "b7ceaefc"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7bdc100",
      "metadata": {
        "id": "b7bdc100"
      },
      "source": [
        "The math module provides access to a variety of mathematical functions, such as `log()` (to\n",
        "produce the natural logarithm of a number), and `sqrt()` (to produce the square root of a\n",
        "number). These functions can be invoked as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b759486",
      "metadata": {
        "id": "5b759486"
      },
      "outputs": [],
      "source": [
        "print(math.log(2.7183))\n",
        "print(math.sqrt(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97414399",
      "metadata": {
        "id": "97414399"
      },
      "source": [
        "#### Defining functions\n",
        "In addition to using built-in functions and functions imported from modules, you should be\n",
        "able to define your own functions (or at least recognize function definitions). For\n",
        "example, the following function takes a list of strings as argument and returns the number\n",
        "of strings that end with the substring *ing*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0bf631",
      "metadata": {
        "id": "3f0bf631"
      },
      "outputs": [],
      "source": [
        "def count_ing(strings):\n",
        "    count = 0\n",
        "    for string in strings:\n",
        "        if string.endswith(\"ing\"):\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "words = [\n",
        "    \"coding\", \"is\", \"about\", \"developing\", \"logical\", \"event\", \"sequences\"\n",
        "]\n",
        "print(count_ing(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b226cf5",
      "metadata": {
        "id": "0b226cf5"
      },
      "source": [
        "#### Reading and writing files\n",
        "You should also have basic knowledge of how to read files (although we will discuss this\n",
        "in reasonable detail in chapter {ref}`chp-getting-data`). An example is given below, where\n",
        "we read the file `data/aesop-wolf-dog.txt` and print its contents to our screen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29163ea2",
      "metadata": {
        "id": "29163ea2"
      },
      "outputs": [],
      "source": [
        "f = open(\"data/aesop-wolf-dog.txt\")  # open a file\n",
        "text = f.read()  # read the contents of a file\n",
        "f.close()  # close the connection to the file\n",
        "print(text)  # print the contents of the file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "271b1c67",
      "metadata": {
        "id": "271b1c67"
      },
      "source": [
        "Even if you have mastered all these programming concepts, it is inevitable that you will\n",
        "encounter lines of code that are unfamiliar. We have done our best to explain the code\n",
        "blocks in great detail. So, while this book is *not* an introduction into the basics of\n",
        "programming, it does increase your understanding of programming, and it prepares you to\n",
        "work on your own problems related to data analysis in the humanities.\n",
        "\n",
        "### Packages and data\n",
        "The code examples used later in the book rely on a number of established and\n",
        "frequently used Python packages, such as NumPy, SciPy, Matplotlib, and Pandas. All these\n",
        "packages can be installed through the <span class=\"index\">Python Package Index</span> (<span class=\"index\">PyPI</span>) using the <span class=\"index\">``pip``</span>\n",
        "software which ships with Python. We have taken care to use packages which are mature and\n",
        "actively maintained. Required packages can be installed by executing the following command\n",
        "on the command-line:\n",
        "\n",
        "```\n",
        "python3 -m pip install \"numpy<2,>=1.13\" \"pandas~=1.1\" \"matplotlib<4,>=2.1\" \"lxml>=3.7\" \"nltk>=3.2\" \"beautifulsoup4>=4.6\" \"pypdf2>=1.26\" \"networkx>=2.8\" \"scipy<2,>=0.18\" \"cartopy>=0.19\" \"scikit-learn>=0.19\" \"xlrd<2,>=1.0\" \"mpl-axes-aligner<2,>=1.1\"``\n",
        "```\n",
        "\n",
        "MacOS users *not* using the Anaconda distribution will need to install a few additional\n",
        "dependencies through the package manager for macOS, Homebrew:\n",
        "\n",
        "```\n",
        "# First, follow the instructions on https://brew.sh to install homebrew\n",
        "# After a successful installation of homebrew, execute the following commands:\n",
        "brew install geos proj\n",
        "```\n",
        "\n",
        "In order to install `cartopy`, Linux users not using the Anaconda distribution will need to install two dependencies via their package manager. On Debian-based systems such as Ubuntu, ``sudo apt install libgeos-dev libproj-dev`` will install these required libraries. If you encounter trouble, try installing a version which is known to work with `python3 -m pip install \"cartopy==0.19.0.post1\"`.\n",
        "\n",
        "Datasets featured in this and subsequent chapters have been gathered together and\n",
        "published online.  The <span class=\"index\">datasets</span> are associated with the DOI ``10.5281/zenodo.891264`` and\n",
        "may be downloaded at the address https://doi.org/10.5281/zenodo.891264. All chapters\n",
        "assume that you have downloaded the datasets and have them available in the current\n",
        "working directory (i.e., the directory from which your Python session is started).\n",
        "\n",
        "### Exercises\n",
        "\n",
        "Each chapter ends with a series of exercises which are increasingly difficult. First,\n",
        "there are \"Easy\" exercises, in which we rehearse some basic lessons and programming skills\n",
        "from the chapter. Next are the \"Moderate\" exercises, in which we ask you to deepen the\n",
        "knowledge you have gained in a chapter. In the \"Challenging\" exercises, finally, we\n",
        "challenge you to go one step further, and apply the chapter's concepts to new problems and\n",
        "new datasets. It is okay to skip certain exercises in the first instance and come back to\n",
        "them later, but we recommend that you do all the exercises in the end, because that is the\n",
        "best way to ensure that you have understood the materials.\n",
        "\n",
        "(sec-cooking-chp-introduction)=\n",
        "## An Exploratory Data Analysis of the United States' Culinary History\n",
        "\n",
        "In the remainder of this chapter we venture into a simple form of <span class=\"index\">exploratory data analysis</span>, serving\n",
        "as a primer of the chapters to follow. The term \"exploratory data analysis\" is attributed to\n",
        "mathematician <span class=\"index\">John Tukey</span>, who characterizes it as a research method or approach to encourage the\n",
        "exploration of data collections using simple statistics and graphical representations. These\n",
        "exploratory analyses serve the goal to obtain new perspectives, insights, and hypotheses about a\n",
        "particular domain. Exploratory data analysis is a well-known term, which Tukey (deliberately)\n",
        "vaguely describes as an analysis that \"does not need probability, significance or confidence\", and\n",
        "\"is actively incisive rather than passively descriptive, with real emphasis on the discovery of the\n",
        "unexpected\" {cite:p}`jones:1986`. Thus, exploratory data analysis provides a lot of freedom as to which\n",
        "techniques should be applied. This chapter will introduce a number of commonly used exploratory\n",
        "techniques (e.g., plotting of raw data, plotting simple statistics, and combining plots) all of which\n",
        "aim to assist us in the discovery of patterns and regularities.\n",
        "\n",
        "As our object of investigation, we will analyze a dataset of seventy-six <span class=\"index\">cookbooks</span>, the\n",
        "*Feeding America: The Historic American Cookbook* dataset {cite:p}`feeding-america`. Cookbooks\n",
        "are of particular interest to humanities scholars, historians, and sociologists, as they\n",
        "serve as an important \"lens\" into a culture's material and economic landscape\n",
        "{cite:p}`mitchell:2001,abala:2012`. The <span class=\"index\">*Feeding America*</span> collection was compiled by the\n",
        "Michigan State University Libraries Special Collections (2003), and holds a representative\n",
        "sample of the culinary history of the United States of America, spanning the late eighteenth to\n",
        "the early twentieth century. The oldest cookbook in the collection is Amelia Simmons's *American\n",
        "Cookery* from 1796, which is believed to be the first cookbook written by someone from and\n",
        "*in* the United States. While many recipes in Simmons's work borrow heavily from\n",
        "predominantly British culinary traditions, it is most well-known for its introduction of\n",
        "American ingredients such as corn. Note that almost all of these books were written\n",
        "by women; it is only since the end of the twentieth century that men started to mingle in the\n",
        "cookbook scene. Until the American Civil War started in 1861, cookbook production\n",
        "increased sharply, with publishers in almost all big cities of the United States. The\n",
        "years following the <span class=\"index\">Civil War</span> showed a second rise in the number of printed cookbooks,\n",
        "which, interestingly, exhibits increasing influences of foreign culinary traditions as the\n",
        "result of the \"new immigration\" in the 1880s from, e.g., Catholic and Jewish immigrants\n",
        "from Italy and Russia. A clear example is the youngest cookbook in the collection, written\n",
        "by Bertha Wood in 1922, which, as Wood explains in the preface \"was to compare the foods\n",
        "of other peoples with that of the Americans in relation to health\". The various dramatic\n",
        "events of the early twentieth century, such as World War I and the Great Depression, have\n",
        "further left their mark on the development of culinary America (see {cite:t}`longone:2003` for a\n",
        "more detailed and elaborate discussion of the *Feeding America* project and the history of\n",
        "cookbooks in America).\n",
        "\n",
        "While necessarily incomplete, this brief overview already highlights the complexity of\n",
        "America's cooking history. The main goal of this chapter is to shed light on some\n",
        "important cooking developments, by employing a range of exploratory data analysis\n",
        "techniques. In particular, we will address the following two research questions:\n",
        "\n",
        "1. Which ingredients have fallen out of fashion and which have become popular in the nineteeth\n",
        "   century?\n",
        "2. Can we observe the influence of immigration waves in the *Feeding America* cookbook\n",
        "   collection?\n",
        "\n",
        "Our corpus, the <span class=\"index\">*Feeding America*</span> cookbook dataset, consists of seventy-six files encoded\n",
        "in XML with annotations for \"recipe type\", \"ingredient\", \"measurements\", and \"cooking\n",
        "implements\". Since processing XML is an involved topic (which is postponed to chapter\n",
        "{ref}`chp-getting-data`), we will make use of a simpler, preprocessed comma-separated\n",
        "version, allowing us to concentrate on basics of performing an exploratory data analysis\n",
        "with Python. The chapter will introduce a number of important libraries and packages for\n",
        "doing data analysis in Python. While we will cover just enough to make all Python code\n",
        "understandable, we will gloss over quite a few theoretical and technical details. We ask\n",
        "you not to worry too much about these details, as they will be explained much more\n",
        "systematically and rigorously in the coming chapters.\n",
        "\n",
        "## Cooking with Tabular Data\n",
        "\n",
        "The Python Data Analysis Library \"<span class=\"index\">Pandas</span>\" is the most popular and well-known Python library for\n",
        "(tabular) data manipulation and data analysis. It is packed with features designed to make data\n",
        "analysis efficient, fast, and easy. As such, the library is particularly well-suited for exploratory\n",
        "data analysis. This chapter will merely scratch the surface of Pandas' many functionalities, and we\n",
        "refer the reader to chapter {ref}`chp-working-with-data` for detailed coverage of the library. Let us\n",
        "start by importing the Pandas library and reading the cookbook dataset into memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff695cf4",
      "metadata": {
        "id": "ff695cf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data/feeding-america.csv\", index_col='date')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc317804",
      "metadata": {
        "id": "bc317804"
      },
      "source": [
        "If this code block appears cryptic, rest assured: we will guide you through it step by\n",
        "step. The first line imports the Pandas library. We do that under an alias, `pd` (read:\n",
        "\"import the pandas library *as* pd\"). After importing the library, we use the function\n",
        "<span class=\"index\">`pandas.read_csv()`</span> to load the cookbook dataset. The function `read_csv()` takes a string\n",
        "as argument, which represents the file path to the cookbook dataset. The function returns\n",
        "a so-called <span class=\"index\">`DataFrame`</span> object, consisting of columns and rows---much like a\n",
        "spreadsheet table. This data frame is then stored in the variable `df`.\n",
        "\n",
        "To inspect the first five rows of the returned data frame, we call its `head()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce21435",
      "metadata": {
        "id": "3ce21435"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92dbe6e1",
      "metadata": {
        "id": "92dbe6e1"
      },
      "source": [
        "Each row in the dataset represents a recipe from one of the seventy-six cookbooks, and\n",
        "provides information about, e.g., its origin, ethnic group, recipe class, region, and,\n",
        "finally, the ingredients to make the recipe. Each row has an index number, which, since we\n",
        "loaded the data with `index_col='date'`, is the same as the year of publication of the\n",
        "recipe's cookbook.\n",
        "\n",
        "To begin our exploratory data analysis, let us first extract some basic statistics from the\n",
        "dataset, starting with the number of recipes in the collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5f40dc3",
      "metadata": {
        "id": "b5f40dc3"
      },
      "outputs": [],
      "source": [
        "print(len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7266ef",
      "metadata": {
        "id": "cb7266ef"
      },
      "source": [
        "The function <span class=\"index\">`len()`</span> is a built-in and generic function to compute the length or size of\n",
        "different types of collections (such as strings, lists, and sets). Recipes are categorized\n",
        "according to different recipe classes, such as \"soups\", \"bread and sweets\", and \"vegetable\n",
        "dishes\". To obtain a list of all recipe classes, we access the column `recipe_class`, and\n",
        "subsequently call `unique()` on the returned column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d38de8",
      "metadata": {
        "id": "24d38de8"
      },
      "outputs": [],
      "source": [
        "print(df['recipe_class'].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d520cce",
      "metadata": {
        "id": "2d520cce"
      },
      "source": [
        "Some of these eight recipe classes occur more frequently than others. To obtain insight\n",
        "in the frequency distribution of these classes, we use the `value_counts()` method, which\n",
        "counts how often each unique value occurs. Again, we first retrieve the column\n",
        "`recipe_class` using `df['recipe_class']`, and subsequently call the method\n",
        "`value_counts()` on that column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c4222f",
      "metadata": {
        "id": "96c4222f"
      },
      "outputs": [],
      "source": [
        "df['recipe_class'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46003375",
      "metadata": {
        "id": "46003375"
      },
      "source": [
        "The table shows that \"bread and sweets\" is the most common recipe category, followed by\n",
        "recipes for \"meat, fish, and game\", and so on and so forth. Plotting these values is as\n",
        "easy as calling the method `plot()` on top of the `Series` object returned by\n",
        "`value_counts()`. In the code block\n",
        "below, we set the argument `kind` to `'bar'` to create a <span class=\"index\">bar plot</span>. The color of all bars\n",
        "is set to the first default color. To make the plot slightly more attractive, we set the\n",
        "width of the bars to 0.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3d606b",
      "metadata": {
        "id": "fb3d606b"
      },
      "outputs": [],
      "source": [
        "df['recipe_class'].value_counts().plot(kind='bar', color=\"C0\", width=0.1);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c04ec8",
      "metadata": {
        "id": "39c04ec8"
      },
      "source": [
        "<!-- Figure: Frequency distribution of the eight most frequent recipe classes.\\label{fig-cookbooks-recipe-classes} -->\n",
        "\n",
        "We continue our exploration of the data. Before we can address our first research question\n",
        "about popularity shifts of ingredients, it is important to get an impression of how\n",
        "the data are distributed over time. The following lines of code plot the number of recipes\n",
        "for each attested year in the collection. Pay close attention to the comments following\n",
        "the hashtags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1242ad08",
      "metadata": {
        "id": "1242ad08"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "grouped = df.groupby('date')  # group all rows from the same year\n",
        "recipe_counts = grouped.size()  # compute the size of each group\n",
        "recipe_counts.plot(style='o', xlim=(1810, 1930))  # plot the group size\n",
        "plt.ylabel(\"number of recipes\")  # add a label to the Y-axis\n",
        "plt.xlabel(\"year of publication\") # add a label to the X-axis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a339eea7",
      "metadata": {
        "id": "a339eea7"
      },
      "source": [
        "<!-- Figure: Number of recipes for each attested year in the collection.\\label{fig:cookbooks:recipe-attestations} -->\n",
        "\n",
        "Here, `groupby()` groups all rows from the same year into separate data frames. The size\n",
        "of these groups (i.e., how many recipes are attested in a particular year), then, is\n",
        "extracted by calling `size()`. Finally, these raw counts are plotted by\n",
        "calling `plot()`. (Note that we set the style to `'o'` to plot points instead of a\n",
        "line.) While a clear trend cannot be discerned, a visual inspection of the graph seems to\n",
        "hint at a slight increase in the number of recipes over the years---but further analyses\n",
        "will have to confirm whether any of the trends we might discern are real.\n",
        "\n",
        "(sec-cooking-chp-culinary-taste-trends)=\n",
        "## Taste Trends in Culinary US History\n",
        "\n",
        "Having explored some rudimentary aspects of the dataset, we can move on to our first\n",
        "research question: can we observe some trends in the use of certain ingredients over time?\n",
        "The column \"ingredients\" provides a list of ingredients per recipe, with each ingredient\n",
        "separated by a semicolon. We will transform these ingredient lists into a slightly more\n",
        "convenient format, allowing us to more easily plot their usage frequency over time. Below,\n",
        "we first split the ingredient strings into actual Python lists using\n",
        "`str.split(';')`. Next, we group all recipes from the same year and merge their\n",
        "ingredient lists with `sum()`. By applying <span class=\"index\">`Series.value_counts()`</span> in the next step, we\n",
        "obtain frequency distributions of ingredients for each year. In order to make sure that\n",
        "any frequency increase of ingredients is not simply due to a higher number of recipes, we\n",
        "should normalise the counts by dividing each ingredient count by the number of attested\n",
        "recipes per year. This is done in the last line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c94f37",
      "metadata": {
        "id": "c3c94f37"
      },
      "outputs": [],
      "source": [
        "# split ingredient strings into lists\n",
        "ingredients = df['ingredients'].str.split(';')\n",
        "# group all rows from the same year\n",
        "groups = ingredients.groupby('date')\n",
        "# merge the lists from the same year\n",
        "ingredients = groups.sum()\n",
        "# compute counts per year\n",
        "ingredients = ingredients.apply(pd.Series.value_counts)\n",
        "# normalise the counts\n",
        "ingredients = ingredients.divide(recipe_counts, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482a95ff",
      "metadata": {
        "id": "482a95ff"
      },
      "source": [
        "These lines of code are quite involved. But don't worry: it is not necessary to understand\n",
        "all the details at this point (it will get easier after completing chapter\n",
        "{ref}`chp-working-with-data`). The resulting data frame consists of rows representing a\n",
        "particular year in the collection and columns representing individual ingredients (this\n",
        "format will be revisited in chapter {ref}`chp-vector-space-model`). This allows us to\n",
        "conveniently extract, manipulate, and plot time series for individual ingredients. Calling\n",
        "the `head()` method allows us to inspect the first five rows of the new data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8feb44e7",
      "metadata": {
        "id": "8feb44e7"
      },
      "outputs": [],
      "source": [
        "ingredients.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ffc58d",
      "metadata": {
        "id": "03ffc58d"
      },
      "source": [
        "Using the ingredient data frame, we will first explore the usage of three ingredients,\n",
        "which have been discussed by different culinary historians: tomatoes, baking powder, and\n",
        "nutmeg (see, e.g., {cite:t}`kraig:2013`). Subsequently, we will use it to employ a data-driven\n",
        "exploration technique to automatically find ingredients that might have undergone some\n",
        "development over time.\n",
        "\n",
        "Let us start with tomatoes. While grown and eaten throughout the nineteenth century, tomatoes\n",
        "did not feature often in early nineteenth century American dishes. One of the reasons for this\n",
        "distaste was that tomatoes were considered poisonous, hindering their widespread diffusion\n",
        "in the early 1900s. Over the course of the nineteenth century, tomatoes only gradually became\n",
        "more popular until, around 1880, Livingston created a new tomato breed which transformed\n",
        "the tomato into a commercial crop and started a true tomato craze. At a glance, these\n",
        "developments appear to be reflected in the time series of tomatoes in the *Feeding\n",
        "America* cookbook collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44f5d1f3",
      "metadata": {
        "id": "44f5d1f3"
      },
      "outputs": [],
      "source": [
        "ax = ingredients['tomato'].plot(style='o', xlim=(1810, 1930))\n",
        "ax.set_ylabel(\"fraction of recipes\")\n",
        "ax.set_xlabel(\"year of publication\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d808c6df",
      "metadata": {
        "id": "d808c6df"
      },
      "source": [
        "<!-- Figure: Relative frequency of recipes containing 'tomatoes' per year.\\label{fig:cookbooks:tomato-timeseries} -->\n",
        "\n",
        "Here, `ingredients['tomato']` selects the column \"tomato\" from our ingredients data\n",
        "frame. The column values, then, are used to draw the time series graph. In order to\n",
        "accentuate the rising trend in this plot, we can add a \"least squares line\" as a\n",
        "reference. The following code block implements a utility function, `plot_trend()`, which\n",
        "enhances our time series plots with such a trend line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d439e027",
      "metadata": {
        "id": "d439e027"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "\n",
        "def plot_trend(column, df, line_color='grey', xlim=(1810, 1930)):\n",
        "    slope, intercept, _, _, _ = scipy.stats.linregress(\n",
        "        df.index, df[column].fillna(0).values)\n",
        "    ax = df[column].plot(style='o', label=column)\n",
        "    ax.plot(df.index, intercept + slope * df.index, '--',\n",
        "             color=line_color, label='_nolegend_')\n",
        "    ax.set_ylabel(\"fraction of recipes\")\n",
        "    ax.set_xlabel(\"year of publication\")\n",
        "    ax.set_xlim(xlim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35138a75",
      "metadata": {
        "id": "35138a75"
      },
      "outputs": [],
      "source": [
        "plot_trend('tomato', ingredients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7839b36",
      "metadata": {
        "id": "c7839b36"
      },
      "source": [
        "<!-- Figure: Relative frequency of recipes containing \"tomatoes\" per year with fitted trend line.\\label{fig:cookbooks:tomato-timeseries-trend} -->\n",
        "\n",
        "The last lines of the function `plot_trend()` plots the <span class=\"index\">trend line</span>. To do that, it uses\n",
        "Python's plotting library, <span class=\"index\">Matplotlib</span>, which we imported a while back in this chapter. The plot\n",
        "illustrates why it is often useful to add a trend line. While the individual data points\n",
        "seem to suggest a relatively strong frequency increase, the trend line in fact hints at a\n",
        "more gradual, conservative increase.\n",
        "\n",
        "Another example of an ingredient that went through an interesting development is baking\n",
        "powder. The plot below shows a rapid increase in the usage of baking powder around 1880,\n",
        "just forty years after the first modern version of baking powder was discovered by the\n",
        "British chemist Alfred Bird. Baking powder was used in a manner similar to yeast, but was\n",
        "deemed better because it acts more quickly. The benefits of this increased cooking\n",
        "efficiency is reflected in the rapid turnover of baking powder at the end of the 19th\n",
        "century:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63e3586",
      "metadata": {
        "id": "b63e3586"
      },
      "outputs": [],
      "source": [
        "plot_trend('baking powder', ingredients)\n",
        "plot_trend('yeast', ingredients)\n",
        "plt.legend();  # add a legend to the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a1ac5e",
      "metadata": {
        "id": "25a1ac5e"
      },
      "source": [
        "<!-- Figure: The relative frequency of recipes containing \"baking powder\" and \"yeast\" per year.\\label{fig:cookbooks:bakingpowder-yeast-timeseries-trend} -->\n",
        "\n",
        "The plot shows two clear trends: (i) a gradual decrease of yeast use in the early\n",
        "nineteenth century, which (ii) is succeeded by the rise of baking powder taking over the\n",
        "position of yeast in American kitchens.\n",
        "\n",
        "As a final example, we explore the use of nutmeg. Nutmeg, a relatively small brown seed,\n",
        "is an interesting case as it has quite a remarkable history. The seed was deemed so\n",
        "valuable that, in 1667 at the Treaty of Breda, the Dutch were willing to trade Manhattan\n",
        "for the small Run Island (part of the Banda Islands of Indonesia), and subsequently, in\n",
        "monopolizing its cultivation, the local people of Run were brutally slaughtered by the\n",
        "Dutch colonists. Nutmeg was still extremely expensive in the early nineteenth century,\n",
        "while at the same time highly popular and fashionable in American cooking. However, over\n",
        "the course of the nineteenth century, nutmeg gradually fell out of favor in the American\n",
        "kitchen, giving room to other delicate (and pricy) seasonings. The plot below displays a\n",
        "clear decay in the usage of nutmeg in nineteenth century American cooking, which is in\n",
        "line with these narrative descriptions in culinary history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdab572",
      "metadata": {
        "id": "afdab572"
      },
      "outputs": [],
      "source": [
        "plot_trend('nutmeg', ingredients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af9ad05",
      "metadata": {
        "id": "1af9ad05"
      },
      "source": [
        "<!-- Figure: Relative frequency of recipes containing \"nutmeg\" per year.\\label{fig:cookbooks:nutmeg-timeseries} -->\n",
        "\n",
        "While it is certainly interesting to visualize and observe the trends described above, the\n",
        "examples given above are well-known cases from the literature. Yet, the real\n",
        "interesting purpose of exploratory data analysis is to discover *new* patterns and\n",
        "regularities, revealing *new* questions and hypotheses. As such, adopting a method that\n",
        "does not rely on pre-existing knowledge on the subject matter would be more interesting\n",
        "for an exploratory analysis.\n",
        "\n",
        "One such method is, for instance, measuring the \"<span class=\"index\">distinctiveness</span>\" or \"<span class=\"index\">keyness</span>\" of certain\n",
        "words in a collection. We could, for example, measure which ingredients are distinctive\n",
        "for recipes stemming from northern states of the United States versus those originating from southern\n",
        "states. Similarly, we may be interested in what distinguishes the post-Civil War era from\n",
        "before the war in terms of recipe ingredients. Measuring \"distinctiveness\" requires a\n",
        "definition and formalization of what distinctiveness means. A well-known and commonly used\n",
        "definition for measuring keyness of words is <span class=\"index\">Pearson</span>'s $\\chi^2$ test statistic, which\n",
        "estimates how likely it is that some observed difference (e.g., the usage frequency of\n",
        "tomatoes before and after the Civil War) is the result of chance. The machine learning\n",
        "library [<span class=\"index\">scikit-learn</span>](http://scikit-learn.org/stable/) implements this test statistic in\n",
        "the function <span class=\"index\">`chi2()`</span>. Below, we demonstrate how to use this function to find ingredients\n",
        "distinguishing the pre-Civil War era from postwar times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f401dd1d",
      "metadata": {
        "id": "f401dd1d"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Transform the index into a list of labels, in which each label\n",
        "# indicates whether a row stems from before or after the Civil War:\n",
        "labels = ['Pre-Civil War' if year < 1864 else 'Post-Civil War' for year in ingredients.index]\n",
        "# replace missing values with zero (.fillna(0)),\n",
        "# and compute the chi2 statistic:\n",
        "keyness, _ = chi2(ingredients.fillna(0), labels)\n",
        "# Turn keyness values into a Series, and sort in descending order:\n",
        "keyness = pd.Series(keyness, index=ingredients.columns).sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "804275df",
      "metadata": {
        "id": "804275df"
      },
      "source": [
        "Inspecting the head of the `keyness` series gives us an overview of the *n* ingredients that\n",
        "most sharply distinguish pre- and postwar times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff4e8cc",
      "metadata": {
        "id": "9ff4e8cc"
      },
      "outputs": [],
      "source": [
        "keyness.head(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c6c53c",
      "metadata": {
        "id": "f6c6c53c"
      },
      "source": [
        "The $\\chi^2$ test statistic identifies \"nutmeg\" as the ingredient most clearly\n",
        "distinguishing the two time frames (in line with its popularity decline descibed above and\n",
        "in the literature). \"Mace\" is a spice made from the reddish seed of the nutmeg seed, and\n",
        "as such, it is no surprise to also find it listed high in the ranking of most distinctive\n",
        "ingredients. Another interesting pair is \"baking powder\" and \"pearlash\". We previously\n",
        "related the decay of \"yeast\" to the rise of \"baking powder\", but did not add \"pearlash\" to\n",
        "the picture. Still, it is reasonable to do so, as pearlash (or *potassium carbonate*) can be\n",
        "considered the first chemical leavener. When combined with an acid\n",
        "(e.g., citrus), pearlash produces a chemical reaction with a carbon dioxide by-product,\n",
        "adding a lightness to the product. The introduction of pearlash is generally attributed to\n",
        "the author of America's first cookbook, the aforementioned Amelia Simmons.\n",
        "\n",
        "One thing that should be noted here is that, while $\\chi^2$ gives us an estimation of the\n",
        "keyness of certain ingredients, it does not tell us the direction of this effect,\n",
        "i.e., whether a particular ingredient is distinctive for one or both collections. Below, we\n",
        "will explore the direction of the effects by creating a simple, yet informative\n",
        "visualization (inspired by {cite:t}`kessler:2017`). The visualization is a simple <span class=\"index\">scatterplot</span> in\n",
        "which the X axis represents the frequency of an ingredient in the post-Civil War era and\n",
        "the Y axis represents the frequency prior to the war. Let us first create the plot; then\n",
        "we will explain how to read and interpret it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84cec0e",
      "metadata": {
        "id": "c84cec0e"
      },
      "outputs": [],
      "source": [
        "# step 1: compute summed ingredient counts per year\n",
        "counts = df['ingredients'].str.split(';').groupby(\n",
        "    'date').sum().apply(pd.Series.value_counts).fillna(0)\n",
        "\n",
        "# step 2: construct frequency rankings for pre- and postwar years\n",
        "pre_cw = counts[counts.index < 1864].sum().rank(method='dense', pct=True)\n",
        "post_cw = counts[counts.index > 1864].sum().rank(method='dense', pct=True)\n",
        "\n",
        "# step 3: merge the pre- and postwar data frames\n",
        "rankings = pd.DataFrame({'Pre-Civil War': pre_cw, 'Post-Civil War': post_cw})\n",
        "\n",
        "# step 4: produce the plot\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.scatter(rankings['Post-Civil War'], rankings['Pre-Civil War'],\n",
        "            c=rankings['Pre-Civil War'] - rankings['Post-Civil War'],\n",
        "            alpha=0.7)\n",
        "\n",
        "# Add annotations of the 20 most distinctive ingredients\n",
        "for i, row in rankings.loc[keyness.head(20).index].iterrows():\n",
        "    plt.annotate(i, xy=(row['Post-Civil War'], row['Pre-Civil War']))\n",
        "\n",
        "plt.xlabel(\"Frequency rank post-Civil War\")\n",
        "plt.ylabel(\"Frequency rank pre-Civil War\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f636303c",
      "metadata": {
        "id": "f636303c"
      },
      "source": [
        "<!-- Figure: Scatter plot showing ingredients distinctive for the pre- and post-Civil War era.\\label{fig:cookbooks:ingredient-keyness} -->\n",
        "\n",
        "These lines of code are quite complicated. Again, don't worry if you don't understand\n",
        "everything. You don't need to. Creating the plot involves the following four steps. First,\n",
        "we construct a data frame with unnormalized ingredient counts, which lists how often the\n",
        "ingredients in the collection occur in each year (step 1). In the second step, we split\n",
        "the counts data frame into a pre- and postwar data frame, compute the summed ingredient\n",
        "counts for each era, and, finally, create ranked lists of ingredients based on their\n",
        "summed counts. In the third step, the pre- and postwar data frames are merged into the\n",
        "`rankings` data frame. The remaining lines (step 4) produce the actual plot, which\n",
        "involves a call to <span class=\"index\">`pyplot.scatter()`</span> for creating a scatter\n",
        "plot, and a loop to annotate the plot with labels for the top twenty most distinctive\n",
        "ingredients (<span class=\"index\">`pyplot.annotate()`</span>).\n",
        "\n",
        "The plot should be read as follows: in the top right, we find ingredients frequently used\n",
        "both before and after the war. Infrequent ingredients in both eras are found in the\n",
        "bottom left. The more interesting ingredients are found near the top left and bottom right\n",
        "corners. These ingredients are either used frequently before the war but infrequently\n",
        "after the war (top left), or vice versa (bottom right). Using this information, we can give\n",
        "a more detailed interpretation to the previously computed keyness numbers, and observe\n",
        "that ingredients like pearlash, saleratus, rice water and loaf sugar are distinctive for\n",
        "pre-war times, while olive oil, baking powder, and vanilla are distinctive for the more\n",
        "modern period. We leave it up to you to experiment with different time frame settings as\n",
        "well as highlighting larger numbers of key ingredients. The brief analysis and techniques\n",
        "presented here serve to show the strength of performing simple exploratory data analyses,\n",
        "with which we can assess and confirm existing hypotheses, and potentially raise new\n",
        "questions and directions for further research.\n",
        "\n",
        "(sec-cooking-chp-foreign-cooking-influences)=\n",
        "## America's Culinary Melting Pot\n",
        "\n",
        "Now that we have a better picture of a number of important changes in the use of cooking\n",
        "ingredients, we move on to our second research question: Can we observe the influence of\n",
        "immigration waves in the <span class=\"index\">*Feeding America*</span> cookbook collection?\n",
        "The nineteenth century United States has\n",
        "witnessed multiple waves of immigration. While there were relatively few immigrants in the\n",
        "first three decades, large-scale <span class=\"index\">immigration</span> started in the 1830s with people coming from\n",
        "Britain, Ireland, and Germany. Other immigration milestones include the 1848 Treaty of\n",
        "Guadalupe Hidalgo, providing US citizenship to over 70,000 Mexican residents, and the \"new\n",
        "immigration\" starting around the 1880s, in which millions of (predominantly) Europeans\n",
        "took the big trip to the United States (see, e.g., {cite:t}`themstrom:1980`). In this section, we\n",
        "will explore whether these three waves of immigration have left their mark in the\n",
        "*Feeding America* cookbook collection.\n",
        "\n",
        "```{margin}\n",
        "In the *Feeding America* data, one value which the `ethnicgroup` variable takes on\n",
        "is `oriental`. \"Oriental\" is a word which one is unlikely to encounter in the contemporary\n",
        "university setting, unless one happens to be studying in London at the School of Oriental\n",
        "and African Studies or consulting a book at Bayerische Staatsbibliothek in the \"East\n",
        "European, Oriental, and Asian Reading Room.\" As these examples hint at, the term is not\n",
        "particularly meaningful outside the context of Western colonialism. In his influential\n",
        "account, {cite:t}`said1978orientalism` argues that \"the Orient\" is a fabricated set of\n",
        "representations which constitute and limit discourse about groups of people living outside\n",
        "of Western Europe and North America [{cite:author}`young1990white`\n",
        "{cite:year}`young1990white`, Ch. 7].\n",
        "```\n",
        "\n",
        "The compilers of the *Feeding America* dataset have provided high-quality <span class=\"index\">annotations</span> for\n",
        "the ethnic origins of a large number of recipes. This information is stored in the\n",
        "\"ethnicgroup\" column of the `df` data frame, which we defined in the previous section.\n",
        "We will first investigate to what extent these annotations can help us to identify\n",
        "(increased) influences of foreign cooking traditions. To obtain an overview of the\n",
        "different ethnic groups in the data as well as their distibution, we construct a frequency\n",
        "table using the method `value_counts()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75471271",
      "metadata": {
        "id": "75471271"
      },
      "outputs": [],
      "source": [
        "df['ethnicgroup'].value_counts(dropna=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "457da10f",
      "metadata": {
        "id": "457da10f"
      },
      "source": [
        "Recipes from unknown origin have the value <span class=\"index\">NaN</span> (standing for \"Not a Number\"), and by\n",
        "default, `value_counts()` leaves out NaN values. However, it would be interesting to gain\n",
        "insight into the number of recipes of which the origin is unknown or unclassified. To include these\n",
        "recipes in the overview, the parameter `dropna` was set to `False`. It is clear from the\n",
        "table above that the vast majority of recipes provides no information about their ethnic\n",
        "group. The top known ethnicities include \"Jewish\", \"Creole\", \"French\" and so on. As a\n",
        "simple indicator of increasing foreign cooking influence, we would expect to observe an\n",
        "increase over time in the number of unique ethnic groups. This expectation appears to be\n",
        "confirmed by the graph constructed below, which displays the average number of different\n",
        "ethnic groups attested each year:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7f3187",
      "metadata": {
        "id": "1a7f3187"
      },
      "outputs": [],
      "source": [
        "grouped = df.groupby(level='date')\n",
        "# compute the number of unique ethnic groups per year,\n",
        "# divided by the number of books\n",
        "n_groups = grouped['ethnicgroup'].nunique() / grouped['book_id'].nunique()\n",
        "n_groups.plot(style='o')\n",
        "\n",
        "# add a least squares line as reference\n",
        "slope, intercept, _, _, _ = scipy.stats.linregress(\n",
        "    n_groups.index, n_groups.fillna(0).values)\n",
        "\n",
        "# create the plot\n",
        "plt.plot(\n",
        "    n_groups.index, intercept + slope * n_groups.index, '--', color=\"grey\")\n",
        "plt.xlim(1810, 1930)\n",
        "plt.ylabel(\"Average number of ethnic groups\")\n",
        "plt.xlabel(\"Year of publication\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7012ad",
      "metadata": {
        "id": "5b7012ad"
      },
      "source": [
        "<!-- Figure: Average number of different etnic groups attested in the cookbooks collection.\\label{fig:cookbooks:ethnic-groups-per-year} -->\n",
        "\n",
        "The graph shows a gradual increase in the number of ethnic groups per year, with (small)\n",
        "peaks around 1830, 1850, 1880, and 1900. It is important to stress that these results do\n",
        "not conclusively support the hypothesis of increasing foreign influence in American\n",
        "cookbooks, because it may very well reflect an artifact of our relatively small cookbook\n",
        "dataset. However, the purpose of exploratory data analysis is not to provide conclusive\n",
        "answers, but to point at possibly interesting paths for future research.\n",
        "\n",
        "To obtain a more detailed description of the development sketched above, let us try to\n",
        "zoom in on some of the foreign culinary contributions. We will employ a similar strategy\n",
        "as before and use a <span class=\"index\">keyness analysis</span> to find ingredients\n",
        "distinguishing ethnically annotated from ethnically unannotated recipes. Here, \"<span\n",
        "class=\"index\">keyness</span>\" is defined slightly differently as the difference between\n",
        "two frequency ranks. Consider the plot below and the different coding steps in the\n",
        "following code block:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6fdd57",
      "metadata": {
        "id": "fa6fdd57"
      },
      "outputs": [],
      "source": [
        "# step 1: add a new column indicating for each recipe whether\n",
        "#         we have information about its ethnic group\n",
        "df['foreign'] = df['ethnicgroup'].notnull()\n",
        "\n",
        "# step 2: construct frequency rankings for foreign and general recipes\n",
        "counts = df.groupby('foreign')['ingredients'].apply(\n",
        "    ';'.join).str.split(';').apply(pd.Series.value_counts).fillna(0)\n",
        "\n",
        "foreign_counts = counts.iloc[1].rank(method='dense', pct=True)\n",
        "general_counts = counts.iloc[0].rank(method='dense', pct=True)\n",
        "\n",
        "# step 3: merge the foreign and general data frames\n",
        "rankings = pd.DataFrame({'foreign': foreign_counts, 'general': general_counts})\n",
        "\n",
        "# step 4: compute the keyness of ingredients in foreign recipes\n",
        "#         as the difference in frequency ranks\n",
        "keyness = (rankings['foreign'] - rankings['general']).sort_values(ascending=False)\n",
        "\n",
        "# step 5: produce the plot\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "plt.scatter(rankings['general'], rankings['foreign'],\n",
        "            c=rankings['foreign'] - rankings['general'],\n",
        "            alpha=0.7)\n",
        "\n",
        "for i, row in rankings.loc[keyness.head(10).index].iterrows():\n",
        "    plt.annotate(i, xy=(row['general'], row['foreign']))\n",
        "\n",
        "plt.xlabel(\"Frequency rank general recipes\")\n",
        "plt.ylabel(\"Frequency rank foreign recipes\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c82301b",
      "metadata": {
        "id": "1c82301b"
      },
      "source": [
        "<!-- Figure: Scatter plot showing ingredients distinctive for recipes annotated for ethnicity.\\label{fig:cookbooks:ingredient-keyness-ethnicity} -->\n",
        "\n",
        "The plot is another <span class=\"index\">scatter plot</span>, highlighting a number of ingredients that are distinctive\n",
        "for recipes annotated for ethnicity. Some notable ingredients are \"olive oil\", \"garlic\",\n",
        "\"syou\", and \"mayonnaise\". \"Olive oil\" and \"garlic\" were both likely imported by\n",
        "Mediterranean immmigrants, while \"syou\" (also called soye) was brought in by Chinese\n",
        "newcomers. Finally, while \"mayonnaise\" is a French invention, its introduction in the\n",
        "United States is generally attributed to Richard Hellmann who emigrated from Germany in\n",
        "1903 to New York City. His Hellmann's Mayonnaise with its characteristic blue ribbon\n",
        "jars became highly popular during the interbellum and still is a popular brand in the\n",
        "United States today.\n",
        "\n",
        "In sum, the brief analysis presented here warrants further investigating the connection\n",
        "between historical immigration waves and changing culinary traditions in nineteenth century\n",
        "America. It was shown that over the course of the nineteenth century we see a gradual increase\n",
        "in the number of unique recipe origins, which coincides with important immigration\n",
        "milestones.\n",
        "\n",
        "(sec-cooking-chp-further-reading)=\n",
        "## Further Reading\n",
        "\n",
        "In this chapter, we have presented the broad topic of exploratory data analysis, and\n",
        "applied it to a collection of historical cookbooks from 19th century America. The analyses\n",
        "presented above served to show that computing basic statistics and producing elementary\n",
        "visualizations are important for building intuitions about a particular data collection,\n",
        "and, when used strategically, can aid the formulation of new hypotheses and research\n",
        "questions. Being able to perform such exploratory finger exercises is a crucial skill for\n",
        "any data analyst as it helps to improve and harness data cleaning, data manipulation and\n",
        "data visualization skills. Exploratory data analysis is a crucial first step to validate\n",
        "certain assumptions about and identify particular patterns in data collections, which will\n",
        "inform the understanding of a research problem and help to evaluate which methodologies\n",
        "are needed to solve that problem. In addition to (tentatively) answering a number of\n",
        "research questions, the chapter's exploratory analyses have undoubtedly left many\n",
        "important issues unanswered, while at the same time raising various new questions. In the\n",
        "chapters to follow, we will present a range of statistical and machine learning models\n",
        "with which these issues and questions can be addressed more systematically, rigorously,\n",
        "and critically. As such, the most important guide for further reading we can give in this\n",
        "chapter is to jump into the subsequent chapters."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "formats": "md:myst",
      "text_representation": {
        "extension": ".md",
        "format_name": "myst",
        "format_version": 0.13,
        "jupytext_version": "1.10.3"
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "source_map": [
      16,
      20,
      245,
      247,
      254,
      256,
      264,
      266,
      278,
      282,
      290,
      293,
      297,
      301,
      307,
      311,
      316,
      318,
      323,
      325,
      334,
      339,
      354,
      356,
      362,
      365,
      373,
      385,
      392,
      397,
      517,
      521,
      533,
      535,
      546,
      548,
      556,
      558,
      566,
      568,
      578,
      580,
      590,
      598,
      626,
      637,
      647,
      649,
      666,
      670,
      680,
      694,
      696,
      714,
      718,
      738,
      740,
      764,
      775,
      780,
      782,
      805,
      829,
      896,
      898,
      911,
      928,
      947,
      977
    ],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}